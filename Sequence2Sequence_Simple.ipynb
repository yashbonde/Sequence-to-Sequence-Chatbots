{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # matrix math\n",
    "import nltk # NLP\n",
    "import re # NLP\n",
    "import gensim.models.word2vec as word2vec # word2vec model\n",
    "import multiprocessing # cpu_count\n",
    "import os # for storing and retirieving the files\n",
    "import codecs # for encoding suring opening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import GRUCell\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.contrib import seq2seq as s2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '~/data/chat.txt'\n",
    "f = codecs.open(file_path, 'r', 'utf-8')\n",
    "corpus = u\"\"\n",
    "corpus += f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenising the text\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentencesList = tokenizer.tokenize(corpus)\n",
    "\n",
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n",
    "    words = clean.split()\n",
    "    return words\n",
    "\n",
    "#sentence where each word is tokenized\n",
    "sentences = []\n",
    "for raw_sentence in sentencesList:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yeah', 'i', 'm', 'preparing', 'myself', 'to', 'drop', 'a', 'lot', 'on', 'this', 'man', 'but', 'definitely', 'need', 'something', 'reliable', 'yeah', 'dude', 'i', 'would', 'definitely', 'consider', 'a', 'daniel', 'defence', 'super', 'reliable', 'and', 'they', 'are', 'just', 'bad', 'ass', 'i', 'm', 'about', 'to', 'meet', 'my', 'mans', 'ex', 'friend', 'with', 'benefit', 'tune', 'in', 'next', 'week', 'to', 'see', 'if', 'i', 'have', 'to', 'put', 'hands', 'on', 'i', 'm', 'dead', 'not', 'looking', 'forward', 'to', 'this', 'shouldn', 't', 'the', 'supporter', 's', 'natural', 'answer', 'to', 's', 'hashtag', 'be']\n",
      "['or', 'just', 'insert', 'itl', 'to', 'make']\n",
      "['you', 'want', 'to', 'turn', 'twitter', 'followers', 'into', 'blog', 'readers']\n",
      "['how', 'do', 'you', 'do', 'this']\n",
      "['besides', 'if', 'trump', 'say', 'his', 'condolences', 'it', 'won', 't', 'sound', 'genuine', 'ex', 'dwayne', 'wade', 'cousin', 'it', 'will', 'sound', 'all', 'political', 'and', 'petty', 'yea', 'you', 'right']\n"
     ]
    }
   ],
   "source": [
    "for s in sentences[:5]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*]Total unique tokens: 119510\n"
     ]
    }
   ],
   "source": [
    "# Making dictionary of unique tokens\n",
    "tokens = []\n",
    "for s in sentences:\n",
    "    tokens.extend(s)\n",
    "tokens = sorted(list(set(tokens)))\n",
    "word2id = dict((c,i) for i,c in enumerate(tokens))\n",
    "print('[*]Total unique tokens:',len(word2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# params for word2vec model\n",
    "e_dim = 100 # dimension of the vector that we want\n",
    "workers = multiprocessing.cpu_count() # for multprocessing\n",
    "min_word_count = 1 # minimum number of times a word must come to be registered\n",
    "context_size = 8 # length of context sentence that would be considered\n",
    "downsample = 1e-5 # for words that come too frequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "w2vector = word2vec.Word2Vec(sg=1, seed=1, size = e_dim, workers = workers,\n",
    "                             min_count = min_word_count, window = context_size,\n",
    "                             sample = downsample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No need to perform this part again, simply load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*]size of vocabulary: 119510\n"
     ]
    }
   ],
   "source": [
    "w2vector.build_vocab(sentences)\n",
    "print('[*]size of vocabulary:', w2vector.wv.vocab.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "716953\n",
      "716953\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(w2vector.corpus_count)\n",
    "print(len(sentences))\n",
    "print(w2vector.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13361009"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "w2vector.train(sentences, total_examples = w2vector.corpus_count, epochs = w2vector.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('workout', 0.9794195294380188),\n",
       " ('cleaning', 0.9784203767776489),\n",
       " ('nap', 0.9771929979324341),\n",
       " ('dorm', 0.975080132484436),\n",
       " ('sleepy', 0.9730733633041382),\n",
       " ('stairs', 0.9730415940284729),\n",
       " ('semester', 0.9725555181503296),\n",
       " ('disneyland', 0.9722797870635986),\n",
       " ('hangout', 0.9721447229385376),\n",
       " ('packing', 0.9720941781997681)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vector.wv.most_similar('gym')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bart', 0.9653815031051636),\n",
       " ('cab', 0.9623619318008423),\n",
       " ('parked', 0.9619486331939697),\n",
       " ('rides', 0.9606517553329468),\n",
       " ('ride', 0.9578251838684082),\n",
       " ('garage', 0.9576617479324341),\n",
       " ('lyft', 0.9543236494064331),\n",
       " ('walk', 0.9519018530845642),\n",
       " ('ac', 0.9517987966537476),\n",
       " ('crowded', 0.9516102075576782)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vector.wv.most_similar('drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nearest_similarity_cosmul(start1, end1, end2):\n",
    "    similarities = w2vector.wv.most_similar_cosmul(\n",
    "        positive=[end2, start1],\n",
    "        negative=[end1]\n",
    "    )\n",
    "    start2 = similarities[0][0]\n",
    "    print(\"{0} is related to {1}, as {2} is related to {3}\".format(start1, end1, start2, end2))\n",
    "    return start2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drink is related to drinking, as cars is related to driving\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cars'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_similarity_cosmul('drink', 'drinking', 'driving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model, so that we can use them later\n",
    "import os\n",
    "if not os._exists('trained'):\n",
    "    os.makedirs('trained')\n",
    "\n",
    "w2vector.save(os.path.join('trained', 'w2vector.w2v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a simple Seq2Seq model for chatbot using twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*]Total correspondance(training examples): 358477\n"
     ]
    }
   ],
   "source": [
    "# Making the model as we already have saved the embeddings\n",
    "# we now need to convert the data to numbers for feeding into the model\n",
    "tweets = [sentences[i] for i in range(0,len(sentences),2)]\n",
    "responses = [sentences[i] for i in range(1, len(sentences), 2)]\n",
    "print('[*]Total correspondance(training examples):', len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model\n",
    "w2vector = word2vec.Word2Vec.load(os.path.join('trained', 'w2vector.w2v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_len = [len(s) for s in tweets]\n",
    "decoder_len = [len(s) for s in responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*]Tweets Maximum Length: 186\n",
      "[*]Tweets Minimum Lenght: 0\n",
      "[*]Responses Maximum Length: 190\n",
      "[*]Responses Minimum Length: 0\n"
     ]
    }
   ],
   "source": [
    "print('[*]Tweets Maximum Length:', max(encoder_len))\n",
    "print('[*]Tweets Minimum Lenght:', min(encoder_len))\n",
    "print('[*]Responses Maximum Length:', max(decoder_len))\n",
    "print('[*]Responses Minimum Length:', min(decoder_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_embeddings(sentences, w2v_model, max_len, e_dim):\n",
    "    embedded_sentences = np.zeros([len(sentences), max_len, e_dim])\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if i%30000 == 0:\n",
    "            print('[!]Processed {0} sentences'.format(i+1))\n",
    "        for j, token in enumerate(sentence):\n",
    "            sentences[i][j] = w2v_model.wv[token]\n",
    "    return embedded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!]Processed 1 sentences\n",
      "[!]Processed 30001 sentences\n",
      "[!]Processed 60001 sentences\n",
      "[!]Processed 90001 sentences\n",
      "[!]Processed 120001 sentences\n",
      "[!]Processed 150001 sentences\n",
      "[!]Processed 180001 sentences\n",
      "[!]Processed 210001 sentences\n",
      "[!]Processed 240001 sentences\n",
      "[!]Processed 270001 sentences\n",
      "[!]Processed 300001 sentences\n",
      "[!]Processed 330001 sentences\n"
     ]
    }
   ],
   "source": [
    "tweets_embedded = batch_to_embeddings(tweets, w2vector, max(encoder_len), e_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!]Processed 1 sentences\n",
      "[!]Processed 30001 sentences\n",
      "[!]Processed 60001 sentences\n",
      "[!]Processed 90001 sentences\n",
      "[!]Processed 120001 sentences\n",
      "[!]Processed 150001 sentences\n",
      "[!]Processed 180001 sentences\n",
      "[!]Processed 210001 sentences\n",
      "[!]Processed 240001 sentences\n",
      "[!]Processed 270001 sentences\n",
      "[!]Processed 300001 sentences\n",
      "[!]Processed 330001 sentences\n"
     ]
    }
   ],
   "source": [
    "responses_embedded = np.zeros([len(responses), max(decoder_len)], dtype = np.int32)\n",
    "for i in range(len(responses)):\n",
    "    if i%30000 == 0:\n",
    "        print('[!]Processed {0} sentences'.format(i+1))\n",
    "    for j in range(len(responses[i])):\n",
    "        responses_embedded[i][j] = word2id[responses[i][j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*]Shape of Tweets: (358477, 186, 100)\n",
      "[*]Shape of Responses: (358476, 190)\n"
     ]
    }
   ],
   "source": [
    "print('[*]Shape of Tweets:', tweets_embedded.shape)\n",
    "print('[*]Shape of Responses:', responses_embedded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the model in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declaring the parameters\n",
    "n_hidden = 128\n",
    "n_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declaring the placeholders\n",
    "encoder_inputs = tf.placeholder(shape = [None, None, e_dim], dtype = tf.float32, name = 'encoder_inputs')\n",
    "decoder_inputs = tf.placeholder(shape = [None, None, e_dim], dtype = tf.float32, name = 'decoder_inputs')\n",
    "decoder_outputs = tf.placeholder(shape = [None, None], dtype = tf.int32, name = 'decoder_outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_cell = tf.contrib.rnn.LSTMCell(n_hidden)\n",
    "encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(\n",
    "    encoder_cell, encoder_inputs,\n",
    "    dtype=tf.float32, time_major=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "decoder_hidden_units = 128\n",
    "decoder_cell = tf.contrib.rnn.LSTMCell(decoder_hidden_units)\n",
    "decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(\n",
    "    decoder_cell, decoder_inputs,\n",
    "    initial_state=encoder_final_state,\n",
    "    dtype=tf.float32, time_major=True, scope=\"plain_decoder\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_logits = tf.contrib.layers.linear(decoder_outputs, len(word2id))\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_outputs, depth=len(word2id), dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(358476, 190, 100)\n"
     ]
    }
   ],
   "source": [
    "# Making decoder inputs\n",
    "din_ = np.zeros(shape = [len(responses), max(decoder_len), e_dim], dtype = np.float32)\n",
    "din_[0] = np.ones(shape = [max(decoder_len), e_dim], dtype = np.float32)\n",
    "print(din_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take sometime\n",
    "for i in range(n_epochs):\n",
    "    feed_dict = {encoder_inputs: tweets_embedded, decoder_inputs: din_, decoder_outputs: responses_embedded}\n",
    "    loss, _ = sess.run([loss, train_op], feed_dict = feed_dict)\n",
    "    print('[$]Epoch {0}, Loss {1}'.format(i+1, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-662f4e6e5799>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Saving the model for future use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seq2seq_simple'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Saving the model for future use\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, 'seq2seq_simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
